# ğŸ‡°ğŸ‡· spaCy Korean Language Model Training

A comprehensive repository for training and experimenting with Korean language models using spaCy. This project explores multiple approaches to Korean NLP, including custom NER (Named Entity Recognition) training, tokenization strategies, and pre-trained model integration.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Training Iterations](#training-iterations)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Entity Types](#entity-types)
- [Contributing](#contributing)

## ğŸ¯ Overview

This repository documents multiple experiments in training Korean language models with spaCy. The project focuses on:

- **Named Entity Recognition (NER)** for Korean business documents
- **Custom tokenization** using various Korean morphological analyzers
- **Pre-trained model integration** and fine-tuning
- **Automated labeling** and dataset preparation workflows

### Key Features

- Multiple training approaches documented across 4 iterations
- Integration with Korean tokenizers (KiwiPiePy, MeCab)
- Custom entity types for business/project domain
- Automated entity detection for stock tickers
- Dataset preparation scripts and utilities

## ğŸ“ Project Structure

```
spacy-model-korean/
â”œâ”€â”€ .devcontainer/          # Development container configuration
â”œâ”€â”€ dlp/                    # Data loss prevention related files
â”œâ”€â”€ kiwipiepy/             # KiwiPiePy tokenizer integration
â”‚   â”œâ”€â”€ kiwi-spacy.ipynb
â”‚   â”œâ”€â”€ regex-kiwispacy.ipynb
â”‚   â””â”€â”€ spacy.ipynb
â”œâ”€â”€ pre-trained/           # Pre-trained model experiments
â”‚   â”œâ”€â”€ dlpt.ipynb
â”‚   â””â”€â”€ spacy.ipynb
â”œâ”€â”€ train-i/               # First training iteration
â”‚   â”œâ”€â”€ base_config.cfg
â”‚   â”œâ”€â”€ config.cfg
â”‚   â”œâ”€â”€ sample.json
â”‚   â”œâ”€â”€ spacy.ipynb
â”‚   â””â”€â”€ output/
â”œâ”€â”€ train-ii/              # Second training iteration (automated labeling)
â”‚   â”œâ”€â”€ auto_label.py
â”‚   â”œâ”€â”€ build_docbin.py
â”‚   â”œâ”€â”€ load_raw_data.py
â”‚   â”œâ”€â”€ stock_dict.py
â”‚   â”œâ”€â”€ base_config.cfg
â”‚   â”œâ”€â”€ config.cfg
â”‚   â”œâ”€â”€ spacy2.ipynb
â”‚   â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ train-iii/             # Third training iteration (MeCab integration)
â”‚   â”œâ”€â”€ base_config.cfg
â”‚   â”œâ”€â”€ config.cfg
â”‚   â”œâ”€â”€ dataset.ipynb
â”‚   â”œâ”€â”€ mecab.ipynb
â”‚   â”œâ”€â”€ mecab2.ipynb
â”‚   â”œâ”€â”€ trainmecab.ipynb
â”‚   â””â”€â”€ data/
â”œâ”€â”€ train-iv/              # Fourth training iteration (advanced NER)
â”‚   â”œâ”€â”€ dlp_train_data.py
â”‚   â”œâ”€â”€ ners.ipynb
â”‚   â””â”€â”€ ners.html
â””â”€â”€ README.md
```

## ğŸ”„ Training Iterations

### Train-I: Basic Training Setup
**Approach:** Initial spaCy model training with basic configuration
- Basic `config.cfg` setup for Korean language
- Sample JSON data format
- Simple training pipeline

**Files:**
- `base_config.cfg` / `config.cfg` - spaCy training configuration
- `sample.json` - Example training data
- `spacy.ipynb` - Training notebook

### Train-II: Automated Labeling
**Approach:** Automated entity detection with custom scripts
- Automated stock ticker detection using regex
- DocBin format for efficient training data storage
- Custom entity labeling pipeline

**Key Components:**
- `auto_label.py` - Automatic entity labeling for stock tickers
- `build_docbin.py` - Convert labeled data to spaCy DocBin format
- `stock_dict.py` - Stock ticker dictionary
- `load_raw_data.py` - Raw data loading utilities

**Usage:**
```python
from auto_label import find_entities

text = "ì‚¼ì„±ì „ì ì£¼ê°€ê°€ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤."
entities = find_entities(text)
# Returns: [(0, 4, "TICKER")]
```

### Train-III: MeCab Integration
**Approach:** Korean morphological analysis with MeCab tokenizer
- MeCab-ko integration for improved Korean tokenization
- Custom dictionary building
- Morpheme-level analysis

**Files:**
- `mecab.ipynb` / `mecab2.ipynb` - MeCab setup and experiments
- `trainmecab.ipynb` - Training with MeCab tokenizer
- `dataset.ipynb` - Dataset preparation

### Train-IV: Advanced NER Training
**Approach:** Comprehensive NER training for business documents
- Rich entity types for project management domain
- DLP (Data Loss Prevention) focused training data
- Advanced entity recognition patterns

**Entity Examples:**
- `PROJECT_NAME`: ì•ŒíŒŒ, ë² íƒ€, ê°ë§ˆ, ë¸íƒ€, etc.
- `PROJECT_MANAGER`: ê¹€ë¯¼ìˆ˜, ì •í•˜ëŠ˜, ìœ¤ë„í˜„, etc.
- `PROJECT_DATE`: 2025ë…„ 3ì›” 1ì¼ë¶€í„° 2026ë…„ 12ì›” 31ì¼ê¹Œì§€
- `PROJECT_BUDGET`: 15ì–µ ì›, 22ì–µ ì›, etc.
- `ORG_INTERNAL`: ë§ˆì¼€íŒ…íŒ€, ì¬ë¬´íŒ€, R&Dí˜ì‹ íŒ€, etc.
- `PROJECT_TERM`: ì½”ë“œëª… ë ˆë“œì¡´, ë‚´ë¶€ë“±ê¸‰A, etc.

**Files:**
- `dlp_train_data.py` - Comprehensive training data with 66+ examples
- `ners.ipynb` - NER training notebook
- `ners.html` - Visualization of NER results

## ğŸ”§ Prerequisites

- Python 3.8+
- spaCy 3.x
- Korean language processing libraries (depending on iteration):
  - `kiwipiepy` (for KiwiPiePy integration)
  - `mecab-python3` (for MeCab integration)

## ğŸ“¦ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/rusestudio/spacy-model-korean.git
cd spacy-model-korean
```

2. **Install spaCy:**
```bash
pip install spacy
```

3. **Install Korean tokenizers (choose based on your training iteration):**

For KiwiPiePy:
```bash
pip install kiwipiepy
```

For MeCab:
```bash
pip install mecab-python3
```

4. **Install additional dependencies:**
```bash
pip install jupyter pandas numpy
```

## ğŸš€ Usage

### Quick Start - Training a Model

#### Option 1: Using Pre-configured Training (Train-II)

```bash
cd train-ii

# 1. Prepare your raw data
python load_raw_data.py

# 2. Auto-label entities
python auto_label.py

# 3. Build DocBin format
python build_docbin.py

# 4. Train the model
python -m spacy train config.cfg --output ./output --paths.train ./dataset/train.spacy --paths.dev ./dataset/dev.spacy
```

#### Option 2: Using Jupyter Notebooks

Each training iteration includes Jupyter notebooks for interactive experimentation:

```bash
# Navigate to your chosen training iteration
cd train-i  # or train-ii, train-iii, train-iv

# Start Jupyter
jupyter notebook

# Open the respective .ipynb file
```

### Using the Trained Model

```python
import spacy

# Load your trained model
nlp = spacy.load("./train-ii/output/model-best")

# Process text
text = "í”„ë¡œì íŠ¸ ì•ŒíŒŒëŠ” ì´ ì˜ˆì‚° 15ì–µ ì›ìœ¼ë¡œ ê¹€ë¯¼ìˆ˜ PMì´ ë‹´ë‹¹í•œë‹¤."
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(f"{ent.text} - {ent.label_}")

# Output:
# ì•ŒíŒŒ - PROJECT_NAME
# 15ì–µ ì› - PROJECT_BUDGET
# ê¹€ë¯¼ìˆ˜ - PROJECT_MANAGER
```

### Generating Base Config

If you want to create a new configuration:

```bash
python -m spacy init config config.cfg --lang ko --pipeline ner
```

## ğŸ·ï¸ Entity Types

The models support the following entity types (primarily in Train-IV):

| Entity Type | Description | Example |
|------------|-------------|---------|
| `PROJECT_NAME` | Project names and codenames | ì•ŒíŒŒ, ë² íƒ€, í´ë¼ìš°ë“œ ì „í™˜ |
| `PROJECT_MANAGER` | Project manager names | ê¹€ë¯¼ìˆ˜, ì •í•˜ëŠ˜, ìœ¤ë„í˜„ |
| `PROJECT_DATE` | Project timeline dates | 2025ë…„ 3ì›” 1ì¼ë¶€í„° 2026ë…„ 12ì›” 31ì¼ê¹Œì§€ |
| `PROJECT_BUDGET` | Project budget amounts | 15ì–µ ì›, 22ì–µ ì› |
| `ORG_INTERNAL` | Internal organization names | ë§ˆì¼€íŒ…íŒ€, ì¬ë¬´íŒ€, R&Dí˜ì‹ íŒ€ |
| `PROJECT_TERM` | Project-specific terminology | ì½”ë“œëª… ë ˆë“œì¡´, ë‚´ë¶€ë“±ê¸‰A |
| `TICKER` | Stock ticker symbols | ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤ |

## ğŸ› ï¸ Development

### Using DevContainer

This project includes a `.devcontainer` configuration for consistent development environment:

1. Open the project in VS Code
2. Install the "Remote - Containers" extension
3. Click "Reopen in Container" when prompted

### Training Data Format

Training data follows spaCy's JSON format:

```json
{
  "text": "í”„ë¡œì íŠ¸ ì•ŒíŒŒëŠ” ì´ ì˜ˆì‚° 15ì–µ ì›ìœ¼ë¡œ ê¹€ë¯¼ìˆ˜ PMì´ ë‹´ë‹¹í•œë‹¤.",
  "entities": [
    [5, 8, "PROJECT_NAME"],
    [17, 22, "PROJECT_BUDGET"],
    [25, 28, "PROJECT_MANAGER"]
  ]
}
```

## ğŸ“Š Model Performance

Each training iteration's output includes:
- Training/validation metrics
- Loss curves
- Best model checkpoint
- Evaluation reports

Check the `output/` directory in each training folder for detailed results.

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Additional entity types for different domains
- More training data for improved accuracy
- Integration with other Korean tokenizers
- Performance optimization
- Documentation improvements

## ğŸ“ Notes

- **Train-iii output** is excluded via `.gitignore` due to size
- Large dictionary files (`.dic`, `matrix.def`) are not tracked
- MeCab build files are ignored to reduce repository size
- Each training iteration is self-contained and can be used independently

## ğŸ“„ License

This project is open source. Please check individual dependencies for their licenses.

## ğŸ”— Useful Links

- [spaCy Documentation](https://spacy.io/)
- [KiwiPiePy GitHub](https://github.com/bab2min/kiwipiepy)
- [MeCab-ko](https://bitbucket.org/eunjeon/mecab-ko)
- [spaCy Korean Models](https://spacy.io/models/ko)

---

**Repository:** [https://github.com/rusestudio/spacy-model-korean](https://github.com/rusestudio/spacy-model-korean)
